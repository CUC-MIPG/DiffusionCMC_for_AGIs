{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from diffusers import (\n",
    "    StableDiffusionPipeline,\n",
    "    T2IAdapter,\n",
    "    StableDiffusionAdapterPipeline,\n",
    "    MultiAdapter,\n",
    ")\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot\n",
    "\n",
    "NEGATIVE_PROMPT = 'extra digit, fewer digits, cropped, worst quality, low quality, blurry, pixelated, low resolution, overexposed, underexposed, too dark, too bright, too blurry, too pixelated, too low resolution, too low quality, too high quality, too high resolution, too sharp, too clear, too focused, too contrasty, too saturated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    def __init__(self) -> None:\n",
    "        self.stable_diffusion_device = \"cuda\"\n",
    "        self.controlnet_device = \"cuda\"\n",
    "\n",
    "        self.num_diffusion_steps = 50\n",
    "\n",
    "        self.use_skeleton = False\n",
    "\n",
    "# replace with your paths\n",
    "class dirs:\n",
    "    def __init__(self) -> None:\n",
    "        self.image_src = \"PATH_TO_SOURCE_IMAGE\"\n",
    "        self.text_src = \"PATHS_TO_TEXT_FILE\"\n",
    "        self.structure_src = [\n",
    "            \"PATHS_TO_EDGE_IMAGES\"\n",
    "        ]\n",
    "        self.color_src = [\n",
    "            \"PATHS_TO_COLOR_IMAGES\"\n",
    "        ]\n",
    "        self.out_dir = \"PATH_TO_OUTPUT_DIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMCEditing:\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.init_models()\n",
    "        self.ref_image = None\n",
    "        self.is_openai_available = False\n",
    "        self.setup_seed(9999)\n",
    "\n",
    "    def init_models(self):\n",
    "        if self.args.controlnet_device == 'cpu':\n",
    "            self.data_type = torch.float32\n",
    "        else:\n",
    "            self.data_type = torch.float16\n",
    "\n",
    "        print('\\033[1;33m' + \"Initializing models...\".center(50, '-') + '\\033[0m')\n",
    "\n",
    "        self.ldm = StableDiffusionPipeline.from_pretrained(\n",
    "            \"sd-legacy/stable-diffusion-v1-5\",\n",
    "            torch_dtype=self.data_type,\n",
    "            custom_pipeline=\"lpw_stable_diffusion\",\n",
    "        ).to(self.args.stable_diffusion_device)\n",
    "        self.tokenizer = self.ldm.tokenizer\n",
    "        self.text_encoder = self.ldm.text_encoder\n",
    "        self.ldm.safety_checker = lambda images, clip_input: (images, False)\n",
    "\n",
    "        l2_variants = [\n",
    "            \"TencentARC/t2iadapter_sketch_sd15v2\",\n",
    "            \"TencentARC/t2iadapter_openpose_sd14v1\",\n",
    "                       ]\n",
    "        \n",
    "        self.l2_adapter = T2IAdapter.from_pretrained(\n",
    "            l2_variants[self.args.use_skeleton],\n",
    "            varient=\"fp16\",\n",
    "            torch_dtype=self.data_type\n",
    "        ).to(self.args.controlnet_device)\n",
    "\n",
    "        self.color_adapter = T2IAdapter.from_pretrained(\n",
    "            \"TencentARC/t2iadapter_color_sd14v1\", \n",
    "            varient=\"fp16\",\n",
    "            torch_dtype=self.data_type\n",
    "        ).to(self.args.controlnet_device)\n",
    "           \n",
    "        self.t2i_pipeline_layer3 = StableDiffusionAdapterPipeline(\n",
    "            vae=self.ldm.vae,\n",
    "            text_encoder=self.ldm.text_encoder,\n",
    "            tokenizer=self.ldm.tokenizer,\n",
    "            unet=self.ldm.unet,\n",
    "            adapter=MultiAdapter([self.l2_adapter,self.color_adapter]),\n",
    "            scheduler=self.ldm.scheduler,\n",
    "            safety_checker=self.ldm.safety_checker,\n",
    "            feature_extractor=self.ldm.feature_extractor\n",
    "            )\n",
    "        \n",
    "        print('\\033[1;32m' + \"Model initialization finished!\".center(50, '-') + '\\033[0m')\n",
    "\n",
    "    def setup_seed(self,seed):\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "    def text_to_image_with_multiple_guidance(self, text, image):\n",
    "        assert len(image) > 1, 'Need more than one guidance image'\n",
    "        generated_image = self.t2i_pipeline_layer3(\n",
    "            text,\n",
    "            image,\n",
    "            num_inference_steps = self.args.num_diffusion_steps,\n",
    "            guidance_scale=7.5,\n",
    "            adapter_conditioning_scale=[1,1],\n",
    "            negative_prompt=NEGATIVE_PROMPT\n",
    "            ).images[0]\n",
    "        return generated_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = args()\n",
    "dirs = dirs()\n",
    "processor = CMCEditing(args)\n",
    "\n",
    "if not os.path.exists(dirs.out_dir):\n",
    "    os.makedirs(dirs.out_dir)\n",
    "\n",
    "ref_img = Image.open(dirs.image_src).resize((512,512))\n",
    "ref_name = os.path.basename(dirs.image_src).replace(\".jpg\",\"\")\n",
    "\n",
    "try:\n",
    "    with open(os.path.join(dirs.text_src,ref_name+'.txt')) as f:\n",
    "                image_caption = f.read()\n",
    "except:\n",
    "    raise ValueError('Text not found for:',dirs.image_src, \" ,exiting...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize=(len(dirs.color_src)*5,len(dirs.structure_src)*5))\n",
    "\n",
    "for i, edge in enumerate(dirs.structure_src):\n",
    "\n",
    "    edge_img = Image.open(edge).convert(\"L\").resize((512,512))\n",
    "    edge_name =  os.path.basename(edge).replace(\".png\",\"\")\n",
    "\n",
    "    pyplot.subplot(len(dirs.structure_src),len(dirs.color_src)+1,(i+1)*len(dirs.color_src))\n",
    "\n",
    "    for j,color in enumerate(dirs.color_src):\n",
    "\n",
    "        color_img = Image.open(color).resize((512,512))\n",
    "        color_name = os.path.basename(color).replace(\".png\",\"\")\n",
    "        save_name = f\"layer3_edited_i{ref_name},e{edge_name},C{color_name}.png\"\n",
    "\n",
    "        layer3_recon = processor.text_to_image_with_multiple_guidance(image_caption, [edge_img, color_img])\n",
    "        layer3_recon.save(os.path.join(dirs.out_dir,save_name))\n",
    "        \n",
    "        pyplot.subplot(len(dirs.structure_src),len(dirs.color_src)+1,(i+1)*(j+2))\n",
    "        pyplot.imshow(layer3_recon)\n",
    "        pyplot.title(color_name)\n",
    "        pyplot.axis(\"off\")\n",
    "\n",
    "pyplot.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cmc_coding",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
